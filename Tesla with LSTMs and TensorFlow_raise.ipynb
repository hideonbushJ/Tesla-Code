{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Tesla Throughput Ratio with LSTMs and TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will build a Long Short Term Memory (LSTM) Network to predict the Tesla Throughput Ratio based on a comment about the market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following libraries for our analysis:\n",
    "\n",
    "* numpy - numerical computing library used to work with our data\n",
    "* pandas - data analysis library used to read in our data from csv\n",
    "* tensorflow - deep learning framework used for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import utils as utl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read and View Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we simply read in our data using pandas, pull out our windfarm, frequency and throuhput ratio into numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from csv file\n",
    "data = pd.read_csv(\"df_lower_alldata.csv\").fillna(0)\n",
    "df = data.iloc[:]\n",
    "# get varibles and results\n",
    "variables = (df.iloc[:,2:-1]+1.5)*10\n",
    "results = df.iloc[:,-1]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BLUFF1</th>\n",
       "      <th>CLEMGPWF</th>\n",
       "      <th>HALLWF1</th>\n",
       "      <th>HALLWF2</th>\n",
       "      <th>HDWF1</th>\n",
       "      <th>HDWF2</th>\n",
       "      <th>HDWF3</th>\n",
       "      <th>LKBONNY2</th>\n",
       "      <th>LKBONNY3</th>\n",
       "      <th>NBHWF1</th>\n",
       "      <th>SNOWNTH1</th>\n",
       "      <th>SNOWSTH1</th>\n",
       "      <th>SNOWTWN1</th>\n",
       "      <th>WATERLWF</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Average Frequency (Hz)</th>\n",
       "      <th>Median Frequency (Hz)</th>\n",
       "      <th>Cummulative Frequency (Hz)</th>\n",
       "      <th>Difference Frequency (Hz)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20778.000000</td>\n",
       "      <td>20778.000000</td>\n",
       "      <td>20778.000000</td>\n",
       "      <td>20778.000000</td>\n",
       "      <td>20778.000000</td>\n",
       "      <td>20778.000000</td>\n",
       "      <td>20778.000000</td>\n",
       "      <td>20778.000000</td>\n",
       "      <td>20778.000000</td>\n",
       "      <td>20778.000000</td>\n",
       "      <td>20778.000000</td>\n",
       "      <td>20778.000000</td>\n",
       "      <td>20778.000000</td>\n",
       "      <td>20778.000000</td>\n",
       "      <td>20778.000000</td>\n",
       "      <td>20778.000000</td>\n",
       "      <td>20778.000000</td>\n",
       "      <td>20778.000000</td>\n",
       "      <td>20778.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>207.036509</td>\n",
       "      <td>246.867874</td>\n",
       "      <td>419.691180</td>\n",
       "      <td>322.101596</td>\n",
       "      <td>495.142052</td>\n",
       "      <td>461.794423</td>\n",
       "      <td>493.784933</td>\n",
       "      <td>479.416315</td>\n",
       "      <td>130.774600</td>\n",
       "      <td>603.763897</td>\n",
       "      <td>574.253916</td>\n",
       "      <td>562.652758</td>\n",
       "      <td>449.434224</td>\n",
       "      <td>501.327606</td>\n",
       "      <td>22.972512</td>\n",
       "      <td>515.107004</td>\n",
       "      <td>515.098510</td>\n",
       "      <td>38.828768</td>\n",
       "      <td>14.985230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>170.720842</td>\n",
       "      <td>182.954921</td>\n",
       "      <td>290.909831</td>\n",
       "      <td>222.130729</td>\n",
       "      <td>320.420734</td>\n",
       "      <td>318.936854</td>\n",
       "      <td>333.595021</td>\n",
       "      <td>442.046316</td>\n",
       "      <td>110.660329</td>\n",
       "      <td>413.383635</td>\n",
       "      <td>440.177101</td>\n",
       "      <td>392.120537</td>\n",
       "      <td>322.323606</td>\n",
       "      <td>390.690506</td>\n",
       "      <td>29.590132</td>\n",
       "      <td>0.391277</td>\n",
       "      <td>0.426374</td>\n",
       "      <td>2.313478</td>\n",
       "      <td>0.824185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>8.516800</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.663400</td>\n",
       "      <td>11.633200</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>5.711800</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>513.422333</td>\n",
       "      <td>513.100014</td>\n",
       "      <td>30.599823</td>\n",
       "      <td>12.099991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>67.585600</td>\n",
       "      <td>145.837500</td>\n",
       "      <td>114.100000</td>\n",
       "      <td>188.125000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>111.524050</td>\n",
       "      <td>40.010000</td>\n",
       "      <td>209.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>188.550000</td>\n",
       "      <td>120.963625</td>\n",
       "      <td>147.010000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>514.870333</td>\n",
       "      <td>514.900017</td>\n",
       "      <td>37.299700</td>\n",
       "      <td>14.400024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>161.905000</td>\n",
       "      <td>227.900000</td>\n",
       "      <td>407.250000</td>\n",
       "      <td>311.580000</td>\n",
       "      <td>502.000000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>480.595000</td>\n",
       "      <td>328.557950</td>\n",
       "      <td>92.592600</td>\n",
       "      <td>612.985000</td>\n",
       "      <td>493.000000</td>\n",
       "      <td>533.000000</td>\n",
       "      <td>456.384200</td>\n",
       "      <td>408.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>515.118000</td>\n",
       "      <td>515.099983</td>\n",
       "      <td>38.700180</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>377.751075</td>\n",
       "      <td>419.836800</td>\n",
       "      <td>697.800000</td>\n",
       "      <td>528.900000</td>\n",
       "      <td>779.000000</td>\n",
       "      <td>748.748900</td>\n",
       "      <td>792.342650</td>\n",
       "      <td>786.867225</td>\n",
       "      <td>205.353100</td>\n",
       "      <td>983.818900</td>\n",
       "      <td>1003.000000</td>\n",
       "      <td>918.543450</td>\n",
       "      <td>746.348800</td>\n",
       "      <td>875.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>515.355334</td>\n",
       "      <td>515.400009</td>\n",
       "      <td>40.200386</td>\n",
       "      <td>15.499992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>544.620000</td>\n",
       "      <td>579.150000</td>\n",
       "      <td>921.700000</td>\n",
       "      <td>718.600000</td>\n",
       "      <td>1023.000000</td>\n",
       "      <td>1038.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1592.450000</td>\n",
       "      <td>401.846600</td>\n",
       "      <td>1325.590000</td>\n",
       "      <td>1448.000000</td>\n",
       "      <td>1265.170000</td>\n",
       "      <td>981.701200</td>\n",
       "      <td>1312.060000</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>516.678665</td>\n",
       "      <td>516.699982</td>\n",
       "      <td>59.400330</td>\n",
       "      <td>18.400002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             BLUFF1      CLEMGPWF       HALLWF1       HALLWF2         HDWF1  \\\n",
       "count  20778.000000  20778.000000  20778.000000  20778.000000  20778.000000   \n",
       "mean     207.036509    246.867874    419.691180    322.101596    495.142052   \n",
       "std      170.720842    182.954921    290.909831    222.130729    320.420734   \n",
       "min       13.000000      8.516800      6.600000      9.000000      4.000000   \n",
       "25%       42.000000     67.585600    145.837500    114.100000    188.125000   \n",
       "50%      161.905000    227.900000    407.250000    311.580000    502.000000   \n",
       "75%      377.751075    419.836800    697.800000    528.900000    779.000000   \n",
       "max      544.620000    579.150000    921.700000    718.600000   1023.000000   \n",
       "\n",
       "              HDWF2         HDWF3      LKBONNY2      LKBONNY3        NBHWF1  \\\n",
       "count  20778.000000  20778.000000  20778.000000  20778.000000  20778.000000   \n",
       "mean     461.794423    493.784933    479.416315    130.774600    603.763897   \n",
       "std      318.936854    333.595021    442.046316    110.660329    413.383635   \n",
       "min        3.000000      4.000000      0.663400     11.633200      3.000000   \n",
       "25%      161.000000    178.000000    111.524050     40.010000    209.000000   \n",
       "50%      437.000000    480.595000    328.557950     92.592600    612.985000   \n",
       "75%      748.748900    792.342650    786.867225    205.353100    983.818900   \n",
       "max     1038.000000   1114.000000   1592.450000    401.846600   1325.590000   \n",
       "\n",
       "           SNOWNTH1      SNOWSTH1      SNOWTWN1      WATERLWF   Unnamed: 15  \\\n",
       "count  20778.000000  20778.000000  20778.000000  20778.000000  20778.000000   \n",
       "mean     574.253916    562.652758    449.434224    501.327606     22.972512   \n",
       "std      440.177101    392.120537    322.323606    390.690506     29.590132   \n",
       "min       15.000000     15.000000      5.711800      3.000000     15.000000   \n",
       "25%      166.000000    188.550000    120.963625    147.010000     15.000000   \n",
       "50%      493.000000    533.000000    456.384200    408.000000     15.000000   \n",
       "75%     1003.000000    918.543450    746.348800    875.000000     15.000000   \n",
       "max     1448.000000   1265.170000    981.701200   1312.060000    194.000000   \n",
       "\n",
       "       Average Frequency (Hz)  Median Frequency (Hz)  \\\n",
       "count            20778.000000           20778.000000   \n",
       "mean               515.107004             515.098510   \n",
       "std                  0.391277               0.426374   \n",
       "min                513.422333             513.100014   \n",
       "25%                514.870333             514.900017   \n",
       "50%                515.118000             515.099983   \n",
       "75%                515.355334             515.400009   \n",
       "max                516.678665             516.699982   \n",
       "\n",
       "       Cummulative Frequency (Hz)  Difference Frequency (Hz)  \n",
       "count                20778.000000               20778.000000  \n",
       "mean                    38.828768                  14.985230  \n",
       "std                      2.313478                   0.824185  \n",
       "min                     30.599823                  12.099991  \n",
       "25%                     37.299700                  14.400024  \n",
       "50%                     38.700180                  15.000000  \n",
       "75%                     40.200386                  15.499992  \n",
       "max                     59.400330                  18.400002  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20778.000000\n",
       "mean        27.832102\n",
       "std         23.087233\n",
       "min          0.000100\n",
       "25%          8.906000\n",
       "50%         22.387350\n",
       "75%         41.494775\n",
       "max        100.000000\n",
       "Name: Lower Throughput Ratio, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train, Test, Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we do is split our data into tranining, validation and test sets and observe the size of each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Set Size\n",
      "Train set: \t\t(14544, 19) \n",
      "Validation set: \t(3117, 19) \n",
      "Test set: \t\t(3117, 19)\n"
     ]
    }
   ],
   "source": [
    "train_x, val_x, test_x, train_y, val_y, test_y = utl.train_val_test_split(variables, results, split_frac=0.70)\n",
    "\n",
    "print(\"Data Set Size\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Training our LSTM Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will define a number of functions that will construct the items in our network. We will then use these functions to build and train our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we simply define a function to build TensorFlow Placeholders for our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    \"\"\"\n",
    "    Create the model inputs\n",
    "    \"\"\"\n",
    "    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    keep_prob_ = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs_, labels_, keep_prob_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TensorFlow the word embeddings are represented as possible data size x embedding size matrix and will learn these weights during our training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_layer(inputs_, vocab_size, embed_size):\n",
    "    \"\"\"\n",
    "    Create the embedding layer\n",
    "    \"\"\"\n",
    "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_size), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_)\n",
    "    \n",
    "    return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow makes it extremely easy to build LSTM Layers and stack them on top of each other. We represent each LSTM layer as a BasicLSTMCell and keep these in a list to stack them together later. Here we will define a list with our LSTM layer sizes and the number of layers. \n",
    "\n",
    "We then take each of these LSTM layers and wrap them in a Dropout Layer. Dropout is a regularization technique using in Neural Networks in which any individual node has a probability of “dropping out” of the network during a given iteration of learning. The makes the model more generalizable by ensuring that it is not too dependent on any given nodes. \n",
    "\n",
    "Finally, we stack these layers using a MultiRNNCell, generate a zero initial state and connect our stacked LSTM layer to our word embedding inputs using dynamic_rnn. Here we track the output and the final state of the LSTM cell, which we will need to pass between mini-batches during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_layers(lstm_sizes, embed, keep_prob_, batch_size):\n",
    "    \"\"\"\n",
    "    Create the LSTM layers\n",
    "    \"\"\"\n",
    "    lstms = [tf.contrib.rnn.BasicLSTMCell(size) for size in lstm_sizes]\n",
    "    # Add dropout to the cell\n",
    "    drops = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob_) for lstm in lstms]\n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(drops)\n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    lstm_outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)\n",
    "    \n",
    "    return initial_state, lstm_outputs, cell, final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we get our predictions by passing the final output of the LSTM layers to a linear activation function via a Tensorflow fully connected layer.  we only care to use the final output for making predictions so we pull this out using the [: , -1] indexing on our LSTM outputs and pass it through a linear activation function to make the predictions. We pass then pass these predictions to our mean squared error loss function and use the Adadelta Optimizer to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cost_fn_and_opt(lstm_outputs, labels_, learning_rate):\n",
    "    \"\"\"\n",
    "    Create the Loss function and Optimizer\n",
    "    \"\"\"\n",
    "    predictions = tf.contrib.layers.fully_connected(lstm_outputs[:, -1], 1, activation_fn=tf.keras.activations.linear)\n",
    "    loss = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    optimzer = tf.train.AdadeltaOptimizer(learning_rate).minimize(loss)\n",
    "    return predictions, loss, optimzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define our accuracy metric for assessing the model performance across our training. Accuracy locates between (0,1), it is more accurate when the accuracy approaches 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_accuracy(predictions, labels_):\n",
    "    labels_=tf.to_float(labels_, name='ToFloat')\n",
    "    diff=tf.losses.mean_squared_error(labels_, predictions)\n",
    "    accuracy=diff/10000\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are finally ready to build and train our LSTM Network! First, we call each of our each of the functions we have defined to construct the network. Then we define a Saver to be able to write our model to disk to load for future use. Finally, we call a Tensorflow Session to train the model over a predefined number of epochs using mini-batches. At the end of each epoch we will print the loss, training accuracy and validation accuracy to monitor the results as we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train_network(lstm_sizes, vocab_size, embed_size, epochs, batch_size,\n",
    "                            learning_rate, keep_prob, train_x, val_x, train_y, val_y):\n",
    "    \n",
    "    inputs_, labels_, keep_prob_ = model_inputs()\n",
    "    embed = build_embedding_layer(inputs_, vocab_size, embed_size)\n",
    "    initial_state, lstm_outputs, lstm_cell, final_state = build_lstm_layers(lstm_sizes, embed, keep_prob_, batch_size)\n",
    "    predictions, loss, optimizer = build_cost_fn_and_opt(lstm_outputs, labels_, learning_rate)\n",
    "    accuracy = build_accuracy(predictions, labels_)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        n_batches = len(train_x)//batch_size\n",
    "        for e in range(epochs):\n",
    "            state = sess.run(initial_state)\n",
    "            \n",
    "            train_acc = []\n",
    "            for ii, (x, y) in enumerate(utl.get_batches(train_x, train_y, batch_size), 1):\n",
    "                feed = {inputs_: x,\n",
    "                        labels_: y[:, None],\n",
    "                        keep_prob_: keep_prob,\n",
    "                        initial_state: state}\n",
    "                loss_, state, _,  batch_acc = sess.run([loss, final_state, optimizer, accuracy], feed_dict=feed)\n",
    "                train_acc.append(batch_acc)\n",
    "                \n",
    "                if (ii + 1) % n_batches == 0:\n",
    "                    \n",
    "                    val_acc = []\n",
    "                    val_state = sess.run(lstm_cell.zero_state(batch_size, tf.float32))\n",
    "                    for xx, yy in utl.get_batches(val_x, val_y, batch_size):\n",
    "                        feed = {inputs_: xx,\n",
    "                                labels_: yy[:, None],\n",
    "                                keep_prob_: 1,\n",
    "                                initial_state: val_state}\n",
    "                        val_batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                        val_acc.append(val_batch_acc)\n",
    "                    \n",
    "                    print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                          \"Batch: {}/{}...\".format(ii+1, n_batches),\n",
    "                          \"Train Loss: {:.3f}...\".format(loss_),\n",
    "                          \"Train Accruacy: {:.3f}...\".format(np.mean(train_acc)*(np.mean(train_acc)>0)),\n",
    "                          \"Val Accuracy: {:.3f}\".format(np.mean(val_acc)))\n",
    "    \n",
    "        saver.save(sess, \"checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define our model hyper parameters. We will build a 2 Layer LSTM Newtork with hidden layer sizes of 128 and 64 respectively. We will use an embedding size of 256 and train over 20 epochs with mini-batches of size 256. We will use an initial learning rate of 0.1, though our Adadelta Optimizer will adapt this over time, and a keep probability of 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Inputs and Hyperparameters\n",
    "lstm_sizes = [64, 128]\n",
    "vocab_size = 3200\n",
    "embed_size = 256\n",
    "epochs = 20\n",
    "batch_size = 256\n",
    "learning_rate = 0.1\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now we train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Batch: 56/56... Train Loss: 763.156... Train Accruacy: 0.119... Val Accuracy: 0.086\n",
      "Epoch: 2/20... Batch: 56/56... Train Loss: 601.740... Train Accruacy: 0.075... Val Accuracy: 0.068\n",
      "Epoch: 3/20... Batch: 56/56... Train Loss: 565.046... Train Accruacy: 0.065... Val Accuracy: 0.063\n",
      "Epoch: 4/20... Batch: 56/56... Train Loss: 523.104... Train Accruacy: 0.061... Val Accuracy: 0.059\n",
      "Epoch: 5/20... Batch: 56/56... Train Loss: 508.472... Train Accruacy: 0.059... Val Accuracy: 0.057\n",
      "Epoch: 6/20... Batch: 56/56... Train Loss: 502.852... Train Accruacy: 0.057... Val Accuracy: 0.056\n",
      "Epoch: 7/20... Batch: 56/56... Train Loss: 502.510... Train Accruacy: 0.056... Val Accuracy: 0.055\n",
      "Epoch: 8/20... Batch: 56/56... Train Loss: 501.193... Train Accruacy: 0.056... Val Accuracy: 0.055\n",
      "Epoch: 9/20... Batch: 56/56... Train Loss: 498.781... Train Accruacy: 0.055... Val Accuracy: 0.054\n",
      "Epoch: 10/20... Batch: 56/56... Train Loss: 493.232... Train Accruacy: 0.055... Val Accuracy: 0.054\n",
      "Epoch: 11/20... Batch: 56/56... Train Loss: 494.094... Train Accruacy: 0.055... Val Accuracy: 0.054\n",
      "Epoch: 12/20... Batch: 56/56... Train Loss: 499.355... Train Accruacy: 0.055... Val Accuracy: 0.054\n",
      "Epoch: 13/20... Batch: 56/56... Train Loss: 491.298... Train Accruacy: 0.055... Val Accuracy: 0.054\n",
      "Epoch: 14/20... Batch: 56/56... Train Loss: 480.124... Train Accruacy: 0.055... Val Accuracy: 0.054\n",
      "Epoch: 15/20... Batch: 56/56... Train Loss: 485.248... Train Accruacy: 0.055... Val Accuracy: 0.054\n",
      "Epoch: 16/20... Batch: 56/56... Train Loss: 498.000... Train Accruacy: 0.055... Val Accuracy: 0.054\n",
      "Epoch: 17/20... Batch: 56/56... Train Loss: 495.223... Train Accruacy: 0.055... Val Accuracy: 0.053\n",
      "Epoch: 18/20... Batch: 56/56... Train Loss: 502.186... Train Accruacy: 0.055... Val Accuracy: 0.053\n",
      "Epoch: 19/20... Batch: 56/56... Train Loss: 506.507... Train Accruacy: 0.055... Val Accuracy: 0.053\n",
      "Epoch: 20/20... Batch: 56/56... Train Loss: 504.511... Train Accruacy: 0.055... Val Accuracy: 0.053\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    build_and_train_network(lstm_sizes, vocab_size, embed_size, epochs, batch_size,\n",
    "                            learning_rate, keep_prob, train_x, val_x, train_y, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we want to do is check the model accuracy on our testing data to make sure it is in line with expecations. We build the Computational Graph just like we did before, however, now instead of training we restore our saved model from our checkpoint directory and then run our test data through the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_network(model_dir, batch_size, test_x, test_y):\n",
    "    \n",
    "    inputs_, labels_, keep_prob_ = model_inputs()\n",
    "    embed = build_embedding_layer(inputs_, vocab_size, embed_size)\n",
    "    initial_state, lstm_outputs, lstm_cell, final_state = build_lstm_layers(lstm_sizes, embed, keep_prob_, batch_size)\n",
    "    predictions, loss, optimizer = build_cost_fn_and_opt(lstm_outputs, labels_, learning_rate)\n",
    "    accuracy = build_accuracy(predictions, labels_)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    test_acc = []\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(model_dir))\n",
    "        test_state = sess.run(lstm_cell.zero_state(batch_size, tf.float32))\n",
    "        for ii, (x, y) in enumerate(utl.get_batches(test_x, test_y, batch_size), 1):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y[:, None],\n",
    "                    keep_prob_: 1,\n",
    "                    initial_state: test_state}\n",
    "            batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "            test_acc.append(batch_acc)\n",
    "        print(\"Test Accuracy: {:.3f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\24270\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from checkpoints\\sentiment.ckpt\n",
      "Test Accuracy: 0.054\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    test_network('checkpoints', batch_size, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
